{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07b74ac3",
   "metadata": {},
   "source": [
    "\n",
    "# How to build a fast AI-surrogate starting from a slow simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3951ff15",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Simulator: Given circuit parameters, outputs frequency response\n",
    "Real simulator: 10 seconds per evaluation\n",
    "Goal: ML model that predicts in <1ms\n",
    "\"\"\"\n",
    "\n",
    "Slow simulator (you'll make a fake one)\n",
    "def slow_circuit_simulator(R, L, C, frequencies):\n",
    "    \"\"\"\n",
    "    Computes frequency response of RLC circuit\n",
    "    Pretend this is expensive (add sleep to simulate)\n",
    "    \"\"\"\n",
    "    import time\n",
    "    time.sleep(0.1)  # Simulate expensive computation\n",
    "    \n",
    "    # Simple RLC transfer function\n",
    "    # TODO: Implement H(jω) = 1 / (1 + jωRC - ω²LC)\n",
    "    pass\n",
    "\n",
    "Your tasks:\n",
    "1. Generate training data efficiently (Latin Hypercube Sampling)\n",
    "2. Build surrogate model (predict response from R, L, C, freq)\n",
    "3. Compare speed: surrogate vs simulator\n",
    "4. Validate accuracy on test set\n",
    "5. Implement uncertainty estimation (bonus)\n",
    "\n",
    "\n",
    "First, we'll be generating some data coming from a fake slow simulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5c3e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing useful libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import signal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd506d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161e6ab3",
   "metadata": {},
   "source": [
    "let's define the fake simulator to generate some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "923509bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slow simulator (you'll make a fake one)\n",
    "def slow_circuit_simulator(R, L, C, frequencies):\n",
    "    \"\"\"\n",
    "    Computes frequency response of RLC circuit\n",
    "    Pretend this is expensive (add sleep to simulate)\n",
    "    \"\"\"\n",
    "    #import time\n",
    "    #time.sleep(0.1)  # Simulate expensive computation\n",
    "    \n",
    "    # Simple RLC transfer function\n",
    "    # TODO: Implement H(jω) = 1 / (1 + jωRC - ω²LC)\n",
    "    num = [1]\n",
    "    den = [L*C, R*C, 1]\n",
    "    w = 2 * np.pi * frequencies  # Convert Hz to rad/s\n",
    "\n",
    "    # Create TransferFunction object\n",
    "    system = signal.TransferFunction(num, den)\n",
    "    \n",
    "    # Evaluate frequency response\n",
    "    w, H = signal.freqresp(system, w)\n",
    "\n",
    "    # Extract real and imaginary parts\n",
    "    real_part = np.real(H)\n",
    "    imag_part = np.imag(H)\n",
    "    \n",
    "    return real_part, imag_part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781bf57a",
   "metadata": {},
   "source": [
    "now we can feed the fake simulator with a dataset that would try to fill the whole operational space. In order to do that, we'll be using LHS (Latin Hypercube Sampling). Let's create 50 different circuits (with different R, L, C values) and for all of them simulate the output at 1000 different frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0533117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import qmc\n",
    "\n",
    "# Latin Hypercube Sampling for 3 parameters and frequencies: R, L, C, w\n",
    "\n",
    "sampler = qmc.LatinHypercube(d=3)\n",
    "l_bounds = [1, 1e-6, 1e-9]  # R: 1-100 Ohm, L: 1uH-10mH, C: 1nF-10uF\n",
    "u_bounds = [100, 10e-3, 10e-6]  # R: 1-100 Ohm, L: 1uH-10mH, C: 1nF-10uF\n",
    "n_circuits = 50\n",
    "\n",
    "circuit_parameters = sampler.random(n=n_circuits)\n",
    "\n",
    "circuit_param = pd.DataFrame(circuit_parameters, columns=['R', 'L', 'C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1b29a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_values = np.logspace(3, 6, num=1000)  # Frequencies from 1kHz to 1MHz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedb916a",
   "metadata": {},
   "source": [
    "The inputs are uniformally distributed, we can proceed generating the outputs through the fake simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d82ee7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_real = []\n",
    "output_imag = []\n",
    "data = {\n",
    "    \"R\": [],\n",
    "    \"L\": [],    \n",
    "    \"C\": [],\n",
    "    \"w\": [],    \n",
    "    \"Real\": [],\n",
    "    \"Imag\": []\n",
    "}\n",
    "\n",
    "for index, row in circuit_param.iterrows():\n",
    "    R = row['R']\n",
    "    L = row['L']\n",
    "    C = row['C']\n",
    "\n",
    "    real_part, imag_part = slow_circuit_simulator(R, L, C, w_values)\n",
    "\n",
    "    output_real.append(real_part)\n",
    "    output_imag.append(imag_part)   \n",
    "\n",
    "    for i in range(len(w_values)): \n",
    "        data[\"R\"].append(R)\n",
    "        data[\"L\"].append(L)\n",
    "        data[\"C\"].append(C)\n",
    "        data[\"w\"].append(w_values[i])\n",
    "        data[\"Real\"].append(real_part[i])\n",
    "        data[\"Imag\"].append(imag_part[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad25c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28348ad1",
   "metadata": {},
   "source": [
    "Okay, now we have generated the outputs from the fake slow simulator, we can start using the data to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "355767c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_names = [\"R\", \"L\", \"C\", \"w\"]\n",
    "output_names = [\"Real\", \"Imag\"]\n",
    "\n",
    "X = data_df[features_names].values\n",
    "y = data_df[output_names].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf22e06",
   "metadata": {},
   "source": [
    "now let's create batches using a DataLoader function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ad79a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), \n",
    "                              torch.tensor(y_train, dtype=torch.float32))\n",
    "\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                torch.tensor(y_test, dtype=torch.float32))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64, \n",
    "    shuffle=True, \n",
    "    pin_memory=True,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                        batch_size=64,\n",
    "                        shuffle=False,\n",
    "                        pin_memory=True,\n",
    "                        num_workers=4,\n",
    "                        persistent_workers=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25614d8a",
   "metadata": {},
   "source": [
    "now we have the data loaders ready, let's define a first architecture for the model surrogate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b351edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurrogateModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SurrogateModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "input_dim = len(features_names)  # 4: R, L, C, w\n",
    "output_dim = len(output_names)  # 2: Real, Imag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9476853",
   "metadata": {},
   "source": [
    "now let's choose an optimizer. For this optimization problem, I would use an STD (Stochastic Gradient Descent) algorithm. Thus, we can select Adam since it is a Gradient-Descent based optimization algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2440859c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/70], Train Loss: 286037.808475, Val Loss: 1.864548\n",
      "Epoch [10/70], Train Loss: 1767.984526, Val Loss: 18.394371\n",
      "Epoch [20/70], Train Loss: 0.003217, Val Loss: 0.003174\n",
      "Epoch [30/70], Train Loss: 0.000730, Val Loss: 0.000552\n",
      "Epoch [40/70], Train Loss: 0.000000, Val Loss: 0.000000\n",
      "Epoch [50/70], Train Loss: 0.000000, Val Loss: 0.000000\n",
      "Epoch [60/70], Train Loss: 0.000000, Val Loss: 0.000000\n",
      "Epoch [70/70], Train Loss: 0.000000, Val Loss: 0.000000\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import mlflow\n",
    "\n",
    "model = SurrogateModel(input_dim, hidden_dim=128, output_dim=output_dim).to(device=device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    \n",
    "    n_epochs = 70\n",
    "\n",
    "    mlflow.log_param(\"model_type\", \"SurrogateModel\")\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"learning_rate\", 0.001)\n",
    "    mlflow.log_param(\"batch_size\", 64)\n",
    "    mlflow.log_param(\"n_epochs\", n_epochs)\n",
    " \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        epoch_training_loss = 0.0\n",
    "        epoch_validation_loss = 0.0\n",
    "        history = {\n",
    "            \"train_loss\": [],\n",
    "            \"val_loss\": []}\n",
    "        \n",
    "        for batch_x, batch_y in train_loader:\n",
    "\n",
    "            batch_x = batch_x.to(device=device)\n",
    "            batch_y = batch_y.to(device=device)\n",
    "\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(batch_x)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_training_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for val_x, val_y in test_loader:\n",
    "                val_x = val_x.to(device=device)\n",
    "                val_y = val_y.to(device=device)\n",
    "\n",
    "                val_predictions = model(val_x)\n",
    "                val_loss = criterion(val_predictions, val_y)\n",
    "\n",
    "                epoch_validation_loss += val_loss.item() * val_x.size(0)\n",
    "\n",
    "        epoch_training_loss /= len(train_loader.dataset)\n",
    "        epoch_validation_loss /= len(test_loader.dataset)\n",
    "\n",
    "        history[\"train_loss\"].append(epoch_training_loss)\n",
    "        history[\"val_loss\"].append(epoch_validation_loss)\n",
    "\n",
    "        if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{n_epochs}], \"\n",
    "                  f\"Train Loss: {epoch_training_loss:.6f}, \"\n",
    "                  f\"Val Loss: {epoch_validation_loss:.6f}\")\n",
    "            \n",
    "        mlflow.log_metric(\"train_loss\", epoch_training_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", epoch_validation_loss, step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3565f701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_dir = \"model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, \"surrogate_model.pth\")\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
